{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetricX \n",
    "\n",
    "This notebook is result of me looking inside MT5 and MetricX modeling to learn these models for reimplementation.\n",
    "\n",
    "\n",
    "1. MetricX23 and MetricX24 are both based on MT5 models\n",
    "2. Both have identical model alchitecture. Both concatente inputs into one long sequence. While both work in ref-less and ref-based modes.\n",
    "3. But the way inputs are prepared (or joined) are different. See following:\n",
    "\n",
    "|        | QE | REF-based |\n",
    "| ------ | --- | -------- |\n",
    "| MetricX23 | `candiate: $TEXT source: $TEXT`  | `candidate: $TEXT reference: $TEXT` |\n",
    "| MetricX24 | `source: $TEXT candidate: $TEXT` | `source: $TEXT candidate: $TEXT reference: $TEXT`  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tg/mambaforge/envs/vecalign/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "/mnt/home/tg/work/repos/tahoma/scripts/debug\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# print widely, dont wrap lines\n",
    "torch.set_printoptions(linewidth=240, precision=4)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(DEVICE)\n",
    "\n",
    "# Add the current directory to the path so we can import the model\n",
    "MYDIR = Path.cwd().resolve()\n",
    "print(MYDIR)\n",
    "if str(MYDIR) not in sys.path:\n",
    "    sys.path.append(str(MYDIR))\n",
    "\n",
    "from metricx_model import MT5ForRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/mnt/home/tg/mambaforge/envs/vecalign/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_id = \"google/mt5-base\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "#model_id = 'google/metricx-23-large-v2p0'\n",
    "# model_id = 'google/metricx-24-hybrid-large-v2p6'\n",
    "# model = MT5ForRegression.from_pretrained(model_id)\n",
    "# print(\"model loaded\")\n",
    "\n",
    "data = \"\"\"Good morning\tgood morning.\tGood Morning\n",
    "morning\tgood morning.\tGood Morning!\n",
    "Evening\tgood evening.\tGood evening!\n",
    "Good night\tgood night.\tGood night!\"\"\"\n",
    "data = [x.split('\\t') for x in data.split('\\n')]\n",
    "\n",
    "\n",
    "# id, is_qe\n",
    "model_ids = [\n",
    "    ('google/metricx-24-hybrid-large-v2p6', 0),\n",
    "    ('google/metricx-24-hybrid-large-v2p6', 1),\n",
    "    ('google/metricx-23-large-v2p0', 0),\n",
    "    ('google/metricx-23-qe-large-v2p0', 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(model_id, source=None, candidate=None, reference=None, is_qe=True):\n",
    "    candidate = candidate and candidate.strip()\n",
    "    assert candidate, \"candidate is required\"\n",
    "    source = source and source.strip()\n",
    "    reference = reference and reference.strip()\n",
    "\n",
    "    if 'metricx-24' in model_id:\n",
    "        assert source, f\"source is required for {model_id}\"\n",
    "        text = f\"source: {source} candidate: {candidate}\"\n",
    "        if not is_qe:\n",
    "            assert reference, f\"reference is required for {model_id} ref-based\"\n",
    "            text += f\" reference: {reference}\"\n",
    "        return text\n",
    "\n",
    "    elif 'metricx-23' in model_id:\n",
    "        text = f\"candidate: {candidate}\"\n",
    "        if is_qe:\n",
    "            assert source, f\"source is required for {model_id}  QE\"\n",
    "            text += f\" source: {source}\"\n",
    "        else:\n",
    "            assert reference, \"reference is required for year 23 without QE\"\n",
    "            text += f\" reference: {reference}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_id: {model_id}\")\n",
    "    return text\n",
    "\n",
    "def tokenize_input(text, device=DEVICE):\n",
    "    res = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # remove eos\n",
    "    res['input_ids'] = res['input_ids'][:, :-1]\n",
    "    res['attention_mask'] = res['attention_mask'][:, :-1]\n",
    "    res = {k: v.to(device) for k, v in res.items()}\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Intercept\n",
    "\n",
    "\n",
    "Intercepting all forward calls for the sake of implementing and comparing the activations.\n",
    "\n",
    "\n",
    "Just picking one model and one input for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3308029770851135\n"
     ]
    }
   ],
   "source": [
    "model_id, is_qe = model_ids[0]\n",
    "s, t, r = data[0]\n",
    "try:\n",
    "    # if model is already loaded, delete it to free up memory\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "model = MT5ForRegression.from_pretrained(model_id)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "text = make_input(model_id, source=s, candidate=t, reference=r, is_qe=is_qe)\n",
    "inp = tokenize_input(text)\n",
    "out = model(**inp).predictions[0].item()\n",
    "print(out)\n",
    "\n",
    "\n",
    "log_file = Path('intercept.log')\n",
    "with log_file.open('w') as out:\n",
    "    out.write(f\"Intercepting {model_id} {is_qe and 'QE' or 'Ref-based'}\\n\")\n",
    "    out.write(f\"text:{text}\\n\")\n",
    "    for k,v in inp.items():\n",
    "        out.write(f\"{k}: {v}\\n\")\n",
    "    out.write(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept MT5Stack \t posargs: 0 kwargs: {'return_dict', 'attention_mask', 'input_ids'}\n",
      "Intercept Embedding embed_tokens\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.0\t posargs: 1 kwargs: {'cache_position', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.0.layer.0\t posargs: 1 kwargs: {'output_attentions', 'use_cache', 'attention_mask', 'cache_position'}\n",
      "Intercept MT5LayerNorm block.0.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.0.layer.0.SelfAttention\t posargs: 1 kwargs: {'use_cache', 'mask', 'output_attentions', 'cache_position'}\n",
      "Intercept Linear block.0.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.0.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.0.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Embedding block.0.layer.0.SelfAttention.relative_attention_bias\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.0.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.0.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.0.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.0.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.0.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.0.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.0.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.0.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.0.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.0.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.0.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.1\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.1.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.1.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.1.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.1.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.1.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.1.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.1.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.1.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.1.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.1.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.1.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.1.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.1.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.1.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.1.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.1.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.1.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.2\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.2.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.2.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.2.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.2.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.2.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.2.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.2.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.2.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.2.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.2.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.2.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.2.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.2.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.2.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.2.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.2.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.2.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.3\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.3.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.3.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.3.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.3.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.3.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.3.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.3.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.3.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.3.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.3.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.3.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.3.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.3.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.3.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.3.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.3.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.3.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.4\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.4.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.4.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.4.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.4.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.4.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.4.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.4.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.4.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.4.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.4.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.4.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.4.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.4.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.4.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.4.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.4.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.4.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.5\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.5.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.5.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.5.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.5.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.5.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.5.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.5.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.5.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.5.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.5.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.5.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.5.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.5.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.5.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.5.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.5.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.5.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.6\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.6.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.6.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.6.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.6.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.6.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.6.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.6.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.6.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.6.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.6.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.6.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.6.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.6.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.6.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.6.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.6.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.6.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.7\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.7.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.7.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.7.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.7.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.7.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.7.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.7.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.7.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.7.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.7.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.7.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.7.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.7.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.7.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.7.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.7.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.7.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.8\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.8.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.8.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.8.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.8.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.8.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.8.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.8.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.8.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.8.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.8.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.8.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.8.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.8.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.8.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.8.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.8.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.8.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.9\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.9.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.9.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.9.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.9.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.9.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.9.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.9.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.9.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.9.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.9.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.9.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.9.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.9.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.9.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.9.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.9.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.9.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.10\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.10.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.10.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.10.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.10.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.10.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.10.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.10.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.10.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.10.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.10.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.10.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.10.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.10.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.10.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.10.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.10.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.10.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.11\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.11.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.11.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.11.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.11.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.11.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.11.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.11.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.11.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.11.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.11.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.11.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.11.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.11.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.11.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.11.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.11.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.11.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.12\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.12.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.12.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.12.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.12.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.12.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.12.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.12.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.12.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.12.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.12.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.12.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.12.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.12.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.12.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.12.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.12.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.12.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.13\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.13.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.13.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.13.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.13.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.13.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.13.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.13.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.13.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.13.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.13.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.13.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.13.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.13.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.13.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.13.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.13.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.13.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.14\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.14.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.14.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.14.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.14.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.14.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.14.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.14.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.14.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.14.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.14.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.14.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.14.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.14.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.14.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.14.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.14.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.14.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.15\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.15.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.15.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.15.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.15.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.15.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.15.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.15.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.15.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.15.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.15.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.15.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.15.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.15.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.15.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.15.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.15.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.15.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.16\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.16.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.16.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.16.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.16.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.16.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.16.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.16.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.16.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.16.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.16.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.16.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.16.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.16.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.16.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.16.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.16.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.16.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.17\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.17.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.17.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.17.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.17.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.17.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.17.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.17.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.17.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.17.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.17.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.17.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.17.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.17.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.17.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.17.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.17.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.17.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.18\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.18.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.18.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.18.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.18.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.18.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.18.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.18.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.18.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.18.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.18.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.18.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.18.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.18.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.18.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.18.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.18.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.18.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.19\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.19.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.19.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.19.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.19.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.19.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.19.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.19.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.19.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.19.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.19.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.19.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.19.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.19.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.19.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.19.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.19.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.19.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.20\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.20.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.20.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.20.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.20.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.20.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.20.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.20.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.20.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.20.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.20.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.20.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.20.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.20.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.20.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.20.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.20.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.20.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.21\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.21.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.21.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.21.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.21.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.21.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.21.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.21.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.21.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.21.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.21.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.21.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.21.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.21.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.21.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.21.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.21.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.21.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.22\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.22.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.22.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.22.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.22.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.22.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.22.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.22.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.22.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.22.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.22.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.22.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.22.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.22.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.22.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.22.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.22.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.22.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Block block.23\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'return_dict', 'attention_mask'}\n",
      "Intercept MT5LayerSelfAttention block.23.layer.0\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'output_attentions', 'use_cache', 'attention_mask'}\n",
      "Intercept MT5LayerNorm block.23.layer.0.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5Attention block.23.layer.0.SelfAttention\t posargs: 1 kwargs: {'cache_position', 'position_bias', 'mask', 'output_attentions', 'use_cache'}\n",
      "Intercept Linear block.23.layer.0.SelfAttention.q\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.23.layer.0.SelfAttention.k\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.23.layer.0.SelfAttention.v\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.23.layer.0.SelfAttention.o\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.23.layer.0.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerFF block.23.layer.1\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm block.23.layer.1.layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept MT5DenseGatedActDense block.23.layer.1.DenseReluDense\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.23.layer.1.DenseReluDense.wi_0\t posargs: 1 kwargs: set()\n",
      "Intercept NewGELUActivation block.23.layer.1.DenseReluDense.act\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.23.layer.1.DenseReluDense.wi_1\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.23.layer.1.DenseReluDense.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Linear block.23.layer.1.DenseReluDense.wo\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout block.23.layer.1.dropout\t posargs: 1 kwargs: set()\n",
      "Intercept MT5LayerNorm final_layer_norm\t posargs: 1 kwargs: set()\n",
      "Intercept Dropout dropout\t posargs: 1 kwargs: set()\n",
      "Intercept Embedding embed_tokens\t posargs: 1 kwargs: set()\n",
      "0.3308029770851135\n"
     ]
    }
   ],
   "source": [
    "#type(model.encoder)\n",
    "import functools\n",
    "#from transformers.models.mt5.modeling_mt5 import MT5Stack, MT5Block\n",
    "\n",
    "def intercepted_forward(self, *args, **kwargs):\n",
    "    non_nulls = {k for k, v in kwargs.items() if v is not None}\n",
    "    _name_ = getattr(self, '_name_', 'unknown')\n",
    "    tag = f'{type(self).__qualname__} {_name_}'\n",
    "    print(f\"Intercept {tag}\\t posargs: {len(args)} kwargs: {non_nulls}\")\n",
    "    tname = None\n",
    "    tensor = None\n",
    "    # guess the most important tensor to log: first posarg or kwarg named 'input_ids'\n",
    "    if 'input_ids' in kwargs:\n",
    "        tensor = kwargs['input_ids']\n",
    "        tname = 'input_ids'\n",
    "    elif 'input' in kwargs:\n",
    "        tensor = kwargs['input']\n",
    "        tname = 'input'\n",
    "    elif args:\n",
    "        tensor = args[0]\n",
    "        tname = 'arg0'\n",
    "    else:\n",
    "        print(\"Could not detect\")\n",
    "    if type(self).__name__ in ('Linear', 'Dropout'):  # skip some basic modules to avoid spam\n",
    "        tensor = None\n",
    "    if tensor is not None:\n",
    "        tag += f' {tname} Shape:[{tensor.shape}] AbsSum: {tensor.abs().sum().item()} max: {tensor.max().item()} min: {tensor.min().item()}'\n",
    "        log_msg = f'\\n\\n{tag}\\n    ' + str(tensor)\n",
    "        with log_file.open('a') as f:\n",
    "            f.write(log_msg)\n",
    "\n",
    "    return self._forward_orig(*args, **kwargs)\n",
    "\n",
    "def intercept_all_forwards(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'forward'):\n",
    "            module._name_ = name\n",
    "            if not hasattr(module, '_forward_orig'):\n",
    "                # avoid recursive interception\n",
    "                module._forward_orig = module.forward\n",
    "            module.forward = functools.partial(intercepted_forward, module)\n",
    "\n",
    "intercept_all_forwards(model.encoder)\n",
    "\n",
    "if True:  # see below\n",
    "    out = model(**inp)\n",
    "    out = out.predictions[0].item()\n",
    "    print(out)\n",
    "    with log_file.open('a') as f:\n",
    "        f.write(f\"Final output: {out}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intercept MT5Attention\n",
    "\n",
    "I didnt get this right. Going to dig deeper via monkey patching to see where my C++ code divereged from this Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "log_file = Path('intercept.attn.log')\n",
    "\n",
    "def forward_debug(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    mask=None,\n",
    "    key_value_states=None,\n",
    "    position_bias=None,\n",
    "    past_key_value=None,\n",
    "    layer_head_mask=None,\n",
    "    query_length=None,\n",
    "    use_cache=False,\n",
    "    output_attentions=False,\n",
    "    cache_position=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n",
    "    \"\"\"\n",
    "    # Input is (batch_size, seq_length, dim)\n",
    "    # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n",
    "    batch_size, seq_length = hidden_states.shape[:2]\n",
    "\n",
    "    # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n",
    "    is_cross_attention = key_value_states is not None\n",
    "\n",
    "    query_states = self.q(hidden_states)\n",
    "    query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        is_updated = past_key_value.is_updated.get(self.layer_idx)\n",
    "        if is_cross_attention:\n",
    "            # after the first generated id, we can subsequently re-use all key/value_states from cache\n",
    "            curr_past_key_value = past_key_value.cross_attention_cache\n",
    "        else:\n",
    "            curr_past_key_value = past_key_value.self_attention_cache\n",
    "\n",
    "    current_states = key_value_states if is_cross_attention else hidden_states\n",
    "    if is_cross_attention and past_key_value is not None and is_updated:\n",
    "        # reuse k,v, cross_attentions\n",
    "        key_states = curr_past_key_value.key_cache[self.layer_idx]\n",
    "        value_states = curr_past_key_value.value_cache[self.layer_idx]\n",
    "    else:\n",
    "        key_states = self.k(current_states)\n",
    "        value_states = self.v(current_states)\n",
    "        key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n",
    "            cache_position = cache_position if not is_cross_attention else None\n",
    "            key_states, value_states = curr_past_key_value.update(\n",
    "                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n",
    "            )\n",
    "            # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n",
    "            if is_cross_attention:\n",
    "                past_key_value.is_updated[self.layer_idx] = True\n",
    "\n",
    "    # compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n",
    "    scores = torch.matmul(query_states, key_states.transpose(3, 2))\n",
    "\n",
    "    if position_bias is None:\n",
    "        key_length = key_states.shape[-2]\n",
    "        # cache position is 0-indexed so we add 1 to get the real length of queries (aka with past)\n",
    "        real_seq_length = query_length if query_length is not None else cache_position[-1] + 1\n",
    "        if not self.has_relative_attention_bias:\n",
    "            position_bias = torch.zeros(\n",
    "                (1, self.n_heads, seq_length, key_length), device=scores.device, dtype=scores.dtype\n",
    "            )\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                position_bias.requires_grad = True\n",
    "        else:\n",
    "            position_bias = self.compute_bias(\n",
    "                real_seq_length, key_length, device=scores.device, cache_position=cache_position\n",
    "            )\n",
    "            position_bias = position_bias[:, :, -seq_length:, :]\n",
    "\n",
    "        if mask is not None:\n",
    "            causal_mask = mask[:, :, :, : key_states.shape[-2]]\n",
    "            print(\">> Causal mask\\n\", causal_mask)\n",
    "            position_bias = position_bias + causal_mask\n",
    "\n",
    "    if self.pruned_heads:\n",
    "        mask = torch.ones(position_bias.shape[1])\n",
    "        mask[list(self.pruned_heads)] = 0\n",
    "        position_bias_masked = position_bias[:, mask.bool()]\n",
    "    else:\n",
    "        position_bias_masked = position_bias\n",
    "\n",
    "    with log_file.open('a') as f:\n",
    "        f.write(f\"\\nMT5Attention Position Bias\\n\" + str(position_bias_masked))\n",
    "\n",
    "    scores += position_bias_masked\n",
    "\n",
    "    # (batch_size, n_heads, seq_length, key_length)\n",
    "    attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(scores)\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "    # Mask heads if we want to\n",
    "    if layer_head_mask is not None:\n",
    "        attn_weights = attn_weights * layer_head_mask\n",
    "    with log_file.open('a') as f:\n",
    "        f.write(f\"\\nMT5Attention attn_weights\\n\" + str(attn_weights))\n",
    "\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
    "    with log_file.open('a') as f:\n",
    "        f.write(f\"\\nMT5Attention attn_output before proj\\n\" + str(attn_output))\n",
    "    attn_output = self.o(attn_output)\n",
    "    with log_file.open('a') as f:\n",
    "        f.write(f\"\\nMT5Attention attn_output after proj\\n\" + str(attn_output))\n",
    "\n",
    "    outputs = (attn_output, past_key_value, position_bias)\n",
    "\n",
    "    if output_attentions:\n",
    "        outputs = outputs + (attn_weights,)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# monkey patch the forward method of the attention layer\n",
    "from transformers.models.mt5.modeling_mt5 import MT5Attention\n",
    "MT5Attention.forward = forward_debug\n",
    "if False:\n",
    "    out = model(**inp)\n",
    "    out = out.predictions[0].item()\n",
    "    print(out)\n",
    "    log_file.open('a').write(f\"\\n\\nFinal output: {out}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------\n",
    "\n",
    "## Scores From All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== google/metricx-24-hybrid-large-v2p6 Ref-based====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3308\tsource: Good morning candidate: good morning. reference: Good Morning\n",
      "0.0000\tsource: morning candidate: good morning. reference: Good Morning!\n",
      "0.0674\tsource: Evening candidate: good evening. reference: Good evening!\n",
      "0.2105\tsource: Good night candidate: good night. reference: Good night!\n",
      "==== google/metricx-24-hybrid-large-v2p6 QE====\n",
      "0.4040\tsource: Good morning candidate: good morning.\n",
      "0.2129\tsource: morning candidate: good morning.\n",
      "0.6364\tsource: Evening candidate: good evening.\n",
      "0.3974\tsource: Good night candidate: good night.\n",
      "==== google/metricx-23-large-v2p0 Ref-based====\n",
      "0.1915\tcandidate: good morning. reference: Good Morning\n",
      "0.1159\tcandidate: good morning. reference: Good Morning!\n",
      "0.1934\tcandidate: good evening. reference: Good evening!\n",
      "0.1730\tcandidate: good night. reference: Good night!\n",
      "==== google/metricx-23-qe-large-v2p0 QE====\n",
      "0.7389\tcandidate: good morning. source: Good morning\n",
      "4.9070\tcandidate: good morning. source: morning\n",
      "0.6479\tcandidate: good evening. source: Evening\n",
      "0.9915\tcandidate: good night. source: Good night\n"
     ]
    }
   ],
   "source": [
    "for model_id, is_qe in model_ids:\n",
    "    model = MT5ForRegression.from_pretrained(model_id)\n",
    "    model = model.to(DEVICE)\n",
    "    texts = []\n",
    "    print(f\"==== {model_id} {is_qe and 'QE' or 'Ref-based'}====\")\n",
    "    for s, m, r in data:\n",
    "        text = make_input(model_id, source=s, candidate=m, reference=r, is_qe=is_qe)\n",
    "        texts.append(text)\n",
    "\n",
    "        inp = tokenize_input(text)\n",
    "        #print(inp)\n",
    "        out = model(**inp)\n",
    "        score = out.predictions[0].item()\n",
    "        print(f'{score:.4f}\\t{text}')\n",
    "    del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### MT5\n",
    "\n",
    "This didnt work as expected or my assumptions of how the model works is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/mt5-base\"\n",
    "tokenizer = transformers.T5Tokenizer.from_pretrained(model_id)\n",
    "pipe = transformers.pipeline(\"translation\", model=model_id, tokenizer=tokenizer)\n",
    "model = pipe.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation_text': '<extra_id_0> good.'}\n",
      "{'translation_text': '<extra_id_0> -'}\n",
      "{'translation_text': '<extra_id_0> ssb'}\n",
      "{'translation_text': '<extra_id_0>, a few weeks'}\n"
     ]
    }
   ],
   "source": [
    "inputs = [\n",
    "    \"translate English to German: That is good.\",\n",
    "    \"cola sentence: The course is jumping well.\",\n",
    "    \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\",\n",
    "    \"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi\"\n",
    "]\n",
    "outputs = pipe(inputs, max_length=64)\n",
    "for out in outputs:\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vecalign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
