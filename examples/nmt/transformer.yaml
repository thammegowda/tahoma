model:
  name: transformer
  args:
    model_dim: 512
    src_vocab_size: 8000
    tgt_vocab_size: 8000
    encoder_layers: 6
    decoder_layers: 3
    attn_head: 8
    ffn_dim: 2048
    dropout: 0.1

optimizer:
  name: adam
  args:
    betas:
      - 0.9
      - 0.98
    eps: 1e-9
    weight_decay: 0.0001

schema:
  fields:
    - sequence
    - sequence
  vocabs:
    - data/vocab.joint.8k.model
    - data/vocab.joint.8k.model
  shared_vocabs: true

trainer:
  data:
    - data/train.kan
    - data/train.eng
  batch_size: 2
  epochs: 25
  max_length:
    - 80
    - 80
  max_length_crop: true

validator:
  data:
    - data/dev.deu
    - data/dev.eng
  batch_size: 64
  max_length:
    - 200
    - 200
  max_length_crop: true
