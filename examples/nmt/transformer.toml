
[model]
name = "transformer"
model_dim = 512
src_vocab_size = 16_000
tgt_vocab_size = 16_000
encoder_layers = 12
decoder_layers = 3
attn_head = 8
ffn_dim = 2048
dropout = 0.1

[optimizer]
name = "adam"
betas = [0.9, 0.98]
eps = 1e-9
weight_decay = 0.0001

[trainer]
batch_size = 64
epochs = 100

[data]
train = ["data/train.deu", "data/train.eng"]
validation = ["data/dev.deu",  "data/dev.eng"]
vocabulary = ["data/shared.16k.model", "data/shared.16k.model"]
max_length = [100, 100]
max_length_crop = true
